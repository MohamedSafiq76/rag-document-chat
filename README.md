<p align="center">
  <h1 align="center">ğŸ“š RAG Document Chat</h1>
  <p align="center">
    <strong>Chat with your enterprise documents using AI â€” grounded answers, zero hallucination.</strong>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white" alt="Python">
    <img src="https://img.shields.io/badge/Streamlit-1.30+-FF4B4B?logo=streamlit&logoColor=white" alt="Streamlit">
    <img src="https://img.shields.io/badge/LLM-Meta_Llama_3.2-0467DF?logo=meta&logoColor=white" alt="Meta Llama">
    <img src="https://img.shields.io/badge/Vector_DB-ChromaDB-4A154B?logo=databricks&logoColor=white" alt="ChromaDB">
    <img src="https://img.shields.io/badge/License-MIT-green" alt="License">
  </p>
  <p align="center">
    <a href="https://rag-document-chat-mohamedsafiq.streamlit.app/"><img src="https://img.shields.io/badge/ğŸš€_Live_Demo-Click_Here-FF4B4B?style=for-the-badge" alt="Live Demo"></a>
  </p>
</p>

---

## ğŸ¯ Overview

A **production-grade Retrieval-Augmented Generation (RAG)** web application that transforms how you interact with enterprise documents. Upload PDFs, DOCX files, or CSVs, and get AI-powered answers that are **grounded in your actual data** â€” with full source citations for transparency and trust.

> **Why RAG?** Unlike generic chatbots that hallucinate, this system retrieves real information from your documents before generating responses â€” ensuring accuracy, reliability, and compliance-readiness.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“„ **Multi-Format Ingestion** | Upload and parse PDF, DOCX, and CSV files seamlessly |
| ğŸ§  **Semantic Search** | Find relevant information using meaning, not just keywords |
| ğŸ”’ **Strict Mode** | Answers *only* from documents â€” zero hallucination for enterprise/compliance use |
| ğŸ”„ **Hybrid Mode** | Documents first, general knowledge as supplement â€” flexible UX |
| ğŸ“ **Source Citations** | Every answer shows exactly which document, page, and section it came from |
| ğŸ’¬ **Conversational Memory** | Multi-turn chat that remembers context across follow-up questions |
| ğŸ’¾ **Persistent Storage** | Embeddings stored locally in ChromaDB â€” survives app restarts |
| âš¡ **ONNX Embeddings** | Lightweight, fast embedding via ONNX Runtime â€” no GPU or PyTorch required |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DOCUMENT PIPELINE                        â”‚
â”‚                                                                 â”‚
â”‚   ğŸ“„ Upload â”€â”€â–¶ Parse & Extract â”€â”€â–¶ Chunk (500 chars) â”€â”€â–¶ Embed â”‚
â”‚   (PDF/DOCX/CSV)    (pypdf/docx)   (RecursiveTextSplitter) (ONNX)â”‚
â”‚                                                   â”‚             â”‚
â”‚                                                   â–¼             â”‚
â”‚                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                                           â”‚  ChromaDB    â”‚     â”‚
â”‚                                           â”‚  Vector Store â”‚     â”‚
â”‚                                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                  â”‚              â”‚
â”‚                        QUERY PIPELINE            â”‚              â”‚
â”‚                                                  â”‚              â”‚
â”‚   ğŸ’¬ Question â”€â”€â–¶ Semantic Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚        â”‚              â”‚                                         â”‚
â”‚        â”‚              â–¼                                         â”‚
â”‚        â”‚     Top-K Relevant Chunks                              â”‚
â”‚        â”‚              â”‚                                         â”‚
â”‚        â–¼              â–¼                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   â”‚ Prompt Builder              â”‚                               â”‚
â”‚   â”‚ (Context + History + Mode)  â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚              â–¼                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   â”‚ Meta Llama 3.2 (HuggingFace)â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚              â–¼                                                  â”‚
â”‚   ğŸ¤– Grounded Answer + ğŸ“ Source Citations                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Streamlit | Interactive web UI with chat interface |
| **LLM** | Meta Llama 3.2 3B Instruct | Response generation via HuggingFace Inference API |
| **Embeddings** | ONNX MiniLM-L6-v2 | Fast, local semantic embeddings (no GPU needed) |
| **Vector DB** | ChromaDB | Persistent local vector storage & similarity search |
| **Doc Parsing** | PyPDF, python-docx, pandas | Multi-format document extraction |
| **Orchestration** | LangChain | Text splitting, document schemas |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- [HuggingFace API Token](https://huggingface.co/settings/tokens) (free tier works)

### Installation

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd Project\ 3

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure your API token
#    Create a .env file with:
HUGGINGFACEHUB_API_TOKEN=your_token_here

# 4. Launch the application
streamlit run app.py
```

The app will open at `http://localhost:8501`.

---

## ğŸ“– Usage Guide

### Step 1: Upload Documents
Use the sidebar to upload one or more PDF, DOCX, or CSV files. Click **"ğŸš€ Process Documents"** to ingest and embed them into the vector database.

### Step 2: Choose Response Mode

| Mode | Behavior | Best For |
|------|----------|----------|
| ğŸ”’ **Strict** | Answers *only* from documents. Returns "insufficient information" if answer isn't found. | Compliance, legal, auditing |
| ğŸ”„ **Hybrid** | Prioritizes documents, supplements with general knowledge when needed. | Research, exploration, learning |

### Step 3: Ask Questions
Type your question in the chat input. The system will:
1. Search the vector database for relevant document chunks
2. Build a context-aware prompt with conversation history
3. Generate a grounded response via Meta Llama 3.2
4. Display source citations with document name, page, and text snippet

---

## ğŸ“ Project Structure

```
Project 3/
â”œâ”€â”€ app.py              # Streamlit UI â€” chat interface, sidebar, mode toggle
â”œâ”€â”€ ingest.py           # Document parsing & recursive text chunking
â”œâ”€â”€ vector_store.py     # ChromaDB operations â€” embed, search, clear
â”œâ”€â”€ rag_chain.py        # LLM prompting â€” strict/hybrid modes, citations
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env                # HuggingFace API token (not committed)
â”œâ”€â”€ .gitignore          # Ignores sensitive & generated files
â””â”€â”€ chroma_db/          # Persistent vector storage (auto-created)
```

---

## ğŸ”‘ Core Concepts Demonstrated

- **Document Ingestion Pipelines** â€” Multi-format parsing with metadata extraction
- **Semantic Chunking** â€” Recursive text splitting with overlap for context preservation
- **Vector Databases** â€” Persistent embedding storage with ChromaDB
- **Prompt Engineering** â€” Mode-specific system prompts for hallucination control
- **Conversational Memory** â€” Multi-turn chat context passed to the LLM
- **Hallucination Mitigation** â€” Strict mode constrains outputs to document context only
- **Source Attribution** â€” Transparent citations for every response

---

## ğŸ“œ License

This project is open source under the [MIT License](LICENSE).

---

<p align="center">
  <strong>Built with â¤ï¸ using LangChain, ChromaDB, Meta Llama & Streamlit</strong>
</p>
